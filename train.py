"""
Two-stage training pipeline for billet surface classification:

  Stage 1 — Supervised Contrastive Pretraining
    Learn rich surface-texture embeddings using SupCon loss. For each image,
    two heavily-augmented views are generated. The model learns to pull
    same-class surfaces together and push different-class surfaces apart
    in embedding space.

  Stage 2 — Linear Evaluation / Fine-tuning
    Freeze (or lightly fine-tune) the pretrained backbone and train a
    classification head to predict ground vs. unground.

Data sources:
  1. camera_images/       — phone camera tiles (primary)
  2. billet_captures*/    — GelSight Mini camera PNGs (secondary)

Usage:
    python train.py                             # run both stages with defaults
    python train.py --stage 1                   # contrastive pretraining only
    python train.py --stage 2                   # fine-tune only (needs pretrained backbone)
    python train.py --pretrain-epochs 100       # longer pretraining
    python train.py --no-gelsight              # phone images only
"""

import argparse
import os
import random
import json
import time
import sys
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from torchvision import transforms, models
from PIL import Image
from tqdm import tqdm


# ---------------------------------------------------------------------------
#  Configuration
# ---------------------------------------------------------------------------

REPO_ROOT = Path(__file__).resolve().parent

# --- Original data directories ---
PHONE_DIRS = {
    "ground":   REPO_ROOT / "camera_images" / "grinded",
    "unground": REPO_ROOT / "camera_images" / "non grinded",
}

GELSIGHT_DIRS = {
    "ground":   REPO_ROOT / "billet_captures_grinded",
    "unground": REPO_ROOT / "billet_captures",
}

# --- Synthetic data directories (generated by generate_synthetic.py) ---
SYNTH_PHONE_DIRS = {
    "ground":   REPO_ROOT / "synthetic" / "phone" / "grinded",
    "unground": REPO_ROOT / "synthetic" / "phone" / "non grinded",
}

SYNTH_DEPTH_DIRS = {
    "ground":   REPO_ROOT / "synthetic" / "depth" / "grinded",
    "unground": REPO_ROOT / "synthetic" / "depth" / "non grinded",
}

LABEL_MAP = {"ground": 0, "unground": 1}   # 0 = smooth/OK, 1 = needs grinding
CLASS_NAMES = ["ground (smooth)", "unground (rough)"]

IMG_SIZE = 224
EMBED_DIM = 128  # contrastive embedding dimension


# ---------------------------------------------------------------------------
#  Datasets
# ---------------------------------------------------------------------------

class BilletSurfaceDataset(Dataset):
    """Standard dataset: returns (image, label). Used for Stage 2 fine-tuning."""

    def __init__(self, samples: list[tuple[str, int]], transform=None):
        self.samples = samples
        self.transform = transform

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        path, label = self.samples[idx]
        image = Image.open(path).convert("RGB")
        if self.transform:
            image = self.transform(image)
        return image, label


class CrossModalContrastiveDataset(Dataset):
    """
    Cross-modal contrastive dataset: each sample is a (phone, depth) pair
    from the same billet/position. Returns augmented views of both.

    For paired samples:  (phone_aug, depth_aug, label, pair_id)
    For unpaired:        (img_aug1, img_aug2, label, pair_id)  [two views of same image]
    """

    def __init__(self, paired_samples, unpaired_samples=None, transform=None):
        """
        Args:
            paired_samples: list of (phone_path, depth_path, label, pair_id)
            unpaired_samples: list of (path, label, pair_id) for images without a cross-modal partner
            transform: augmentation transform
        """
        self.paired = paired_samples
        self.unpaired = unpaired_samples or []
        self.transform = transform
        self.total = len(self.paired) + len(self.unpaired)

    def __len__(self):
        return self.total

    def __getitem__(self, idx):
        if idx < len(self.paired):
            phone_path, depth_path, label, pair_id = self.paired[idx]
            phone_img = Image.open(phone_path).convert("RGB")
            depth_img = Image.open(depth_path).convert("RGB")
            view1 = self.transform(phone_img) if self.transform else phone_img
            view2 = self.transform(depth_img) if self.transform else depth_img
        else:
            unpaired_idx = idx - len(self.paired)
            path, label, pair_id = self.unpaired[unpaired_idx]
            img = Image.open(path).convert("RGB")
            view1 = self.transform(img) if self.transform else img
            view2 = self.transform(img) if self.transform else img

        return view1, view2, label, pair_id


# ---------------------------------------------------------------------------
#  Data collection helpers
# ---------------------------------------------------------------------------

def parse_billet_position(filename: str):
    """Extract (billet_num, position) from filenames like billet1_camera_3.jpg or billet1_pos3_aug2.jpg."""
    stem = Path(filename).stem
    parts = stem.split("_")
    billet_num = int(parts[0].replace("billet", ""))
    # Handle both original (billet1_camera_3) and synthetic (billet1_pos3_orig / billet1_pos3_aug2)
    for p in parts[1:]:
        if p.startswith("pos"):
            return billet_num, int(p.replace("pos", ""))
        if p.isdigit():
            return billet_num, int(p)
    # Fallback: last numeric part
    for p in reversed(parts):
        cleaned = p.replace("aug", "").replace("orig", "")
        if cleaned.isdigit():
            return billet_num, int(cleaned)
    return billet_num, 0


def collect_cross_modal_pairs(use_synthetic=True, no_gelsight=False):
    """
    Build cross-modal (phone, depth) pairs by matching billet number and position.
    Returns paired_samples, unpaired_samples, stats dict.
    """
    # Collect all phone images indexed by (class, billet, position)
    phone_index = {}
    phone_dirs = dict(PHONE_DIRS)
    if use_synthetic:
        for cls, d in SYNTH_PHONE_DIRS.items():
            if d.exists():
                phone_dirs[f"synth_{cls}"] = d

    for dir_key, directory in phone_dirs.items():
        if not directory.exists():
            continue
        cls = dir_key.replace("synth_", "")
        label = LABEL_MAP[cls]
        for f in sorted(directory.iterdir()):
            if f.suffix.lower() in (".jpg", ".jpeg", ".png"):
                billet, pos = parse_billet_position(f.name)
                key = (cls, billet, pos)
                phone_index.setdefault(key, []).append(str(f))

    # Collect all depth images indexed by (class, billet, position)
    depth_index = {}
    if not no_gelsight:
        depth_dirs = dict(GELSIGHT_DIRS)
        if use_synthetic:
            for cls, d in SYNTH_DEPTH_DIRS.items():
                if d.exists():
                    depth_dirs[f"synth_{cls}"] = d

        for dir_key, directory in depth_dirs.items():
            if not directory.exists():
                continue
            cls = dir_key.replace("synth_", "")
            label = LABEL_MAP[cls]
            for f in sorted(directory.iterdir()):
                if "depth" in f.stem and f.suffix.lower() == ".png":
                    billet, pos = parse_billet_position(f.name)
                    key = (cls, billet, pos)
                    depth_index.setdefault(key, []).append(str(f))
                # Synthetic depth files don't have "depth" in name, handle them too
                elif directory in SYNTH_DEPTH_DIRS.values() and f.suffix.lower() == ".png":
                    billet, pos = parse_billet_position(f.name)
                    key = (cls, billet, pos)
                    depth_index.setdefault(key, []).append(str(f))

    # Build paired and unpaired samples
    all_keys = set(phone_index.keys()) | set(depth_index.keys())
    paired_samples = []  # (phone_path, depth_path, label, pair_id)
    unpaired_samples = []  # (path, label, pair_id)
    pair_id_counter = 0

    for key in sorted(all_keys):
        cls, billet, pos = key
        label = LABEL_MAP[cls]
        phone_files = phone_index.get(key, [])
        depth_files = depth_index.get(key, [])

        if phone_files and depth_files:
            # Create all cross-modal pairs for this position
            for pf in phone_files:
                for df in depth_files:
                    paired_samples.append((pf, df, label, pair_id_counter))
            pair_id_counter += 1
        elif phone_files:
            for pf in phone_files:
                unpaired_samples.append((pf, label, pair_id_counter))
            pair_id_counter += 1
        elif depth_files:
            for df in depth_files:
                unpaired_samples.append((df, label, pair_id_counter))
            pair_id_counter += 1

    stats = {
        "paired": len(paired_samples),
        "unpaired": len(unpaired_samples),
        "total_phone": sum(len(v) for v in phone_index.values()),
        "total_depth": sum(len(v) for v in depth_index.values()),
        "unique_positions": pair_id_counter,
    }

    return paired_samples, unpaired_samples, stats


def collect_all_flat_samples(use_synthetic=True, no_gelsight=False):
    """Collect all images as flat (path, label) list — used for Stage 2 fine-tuning."""
    samples = []
    for cls_name, directory in PHONE_DIRS.items():
        if not directory.exists():
            continue
        label = LABEL_MAP[cls_name]
        for f in sorted(directory.iterdir()):
            if f.suffix.lower() in (".jpg", ".jpeg", ".png"):
                samples.append((str(f), label))

    if use_synthetic:
        for cls_name, directory in SYNTH_PHONE_DIRS.items():
            if not directory.exists():
                continue
            label = LABEL_MAP[cls_name]
            for f in sorted(directory.iterdir()):
                if f.suffix.lower() in (".jpg", ".jpeg", ".png"):
                    samples.append((str(f), label))

    if not no_gelsight:
        for cls_name, directory in GELSIGHT_DIRS.items():
            if not directory.exists():
                continue
            label = LABEL_MAP[cls_name]
            for f in sorted(directory.iterdir()):
                if "depth" in f.stem and f.suffix.lower() == ".png":
                    samples.append((str(f), label))

        if use_synthetic:
            for cls_name, directory in SYNTH_DEPTH_DIRS.items():
                if not directory.exists():
                    continue
                label = LABEL_MAP[cls_name]
                for f in sorted(directory.iterdir()):
                    if f.suffix.lower() == ".png":
                        samples.append((str(f), label))

    return samples


def train_val_split_paired(paired, unpaired, val_ratio=0.2, seed=42):
    """Split paired + unpaired samples into train/val by billet (no data leakage)."""
    rng = random.Random(seed)

    # Group by pair_id to ensure paired samples stay together
    paired_by_id = {}
    for item in paired:
        pid = item[3]
        paired_by_id.setdefault(pid, []).append(item)

    unpaired_by_id = {}
    for item in unpaired:
        pid = item[2]
        unpaired_by_id.setdefault(pid, []).append(item)

    all_ids = sorted(set(paired_by_id.keys()) | set(unpaired_by_id.keys()))
    rng.shuffle(all_ids)

    n_val = max(1, int(len(all_ids) * val_ratio))
    val_ids = set(all_ids[:n_val])

    train_paired, val_paired = [], []
    for pid, items in paired_by_id.items():
        if pid in val_ids:
            val_paired.extend(items)
        else:
            train_paired.extend(items)

    train_unpaired, val_unpaired = [], []
    for pid, items in unpaired_by_id.items():
        if pid in val_ids:
            val_unpaired.extend(items)
        else:
            train_unpaired.extend(items)

    return train_paired, train_unpaired, val_paired, val_unpaired


def train_val_split(samples, val_ratio=0.2, seed=42):
    """Split flat samples into train/val, stratified by label."""
    rng = random.Random(seed)
    by_label = {}
    for path, label in samples:
        by_label.setdefault(label, []).append((path, label))

    train, val = [], []
    for label, items in by_label.items():
        rng.shuffle(items)
        n_val = max(1, int(len(items) * val_ratio))
        val.extend(items[:n_val])
        train.extend(items[n_val:])
    return train, val


def make_weighted_sampler(samples):
    """Create a WeightedRandomSampler to handle class imbalance."""
    labels = [l for _, l in samples]
    class_counts = [labels.count(c) for c in range(2)]
    class_weights = [1.0 / max(c, 1) for c in class_counts]
    sample_weights = [class_weights[l] for l in labels]
    return WeightedRandomSampler(sample_weights, num_samples=len(samples), replacement=True)


# ---------------------------------------------------------------------------
#  Augmentation pipelines
# ---------------------------------------------------------------------------

def get_contrastive_transform():
    """
    Strong augmentation for contrastive learning — must be aggressive enough
    that the model can't rely on trivial shortcuts to match views.
    """
    return transforms.Compose([
        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.2, 1.0), ratio=(0.75, 1.33)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomVerticalFlip(),
        transforms.RandomApply([
            transforms.RandomRotation(45),
        ], p=0.5),
        transforms.RandomApply([
            transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.4, hue=0.15),
        ], p=0.8),
        transforms.RandomGrayscale(p=0.3),
        transforms.RandomApply([
            transforms.GaussianBlur(kernel_size=7, sigma=(0.1, 3.0)),
        ], p=0.5),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225]),
        transforms.RandomErasing(p=0.3, scale=(0.02, 0.25)),
    ])


def get_finetune_transform():
    """Moderate augmentation for fine-tuning stage."""
    return transforms.Compose([
        transforms.RandomResizedCrop(IMG_SIZE, scale=(0.5, 1.0)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.05),
        transforms.RandomGrayscale(p=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225]),
    ])


def get_val_transform():
    """Deterministic transform for validation."""
    return transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(IMG_SIZE),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225]),
    ])


# ---------------------------------------------------------------------------
#  Model components
# ---------------------------------------------------------------------------

class ProjectionHead(nn.Module):
    """MLP projection head for contrastive learning (maps features -> embedding)."""

    def __init__(self, in_dim, hidden_dim=512, out_dim=EMBED_DIM):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, out_dim),
        )

    def forward(self, x):
        return F.normalize(self.net(x), dim=1)


class ContrastiveModel(nn.Module):
    """
    EfficientNet-B0 backbone + projection head for contrastive pretraining.
    The backbone produces feature vectors; the projection head maps them to
    a normalized embedding space where the contrastive loss operates.
    """

    def __init__(self):
        super().__init__()
        backbone = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)
        self.feature_dim = backbone.classifier[1].in_features
        backbone.classifier = nn.Identity()
        self.backbone = backbone
        self.projector = ProjectionHead(self.feature_dim, out_dim=EMBED_DIM)

    def forward(self, x):
        features = self.backbone(x)
        embeddings = self.projector(features)
        return features, embeddings


class Classifier(nn.Module):
    """Linear classifier head trained on frozen/fine-tuned backbone features."""

    def __init__(self, in_dim, num_classes=2):
        super().__init__()
        self.head = nn.Sequential(
            nn.Dropout(p=0.3),
            nn.Linear(in_dim, num_classes),
        )

    def forward(self, x):
        return self.head(x)


# ---------------------------------------------------------------------------
#  Supervised Contrastive Loss (SupCon)
# ---------------------------------------------------------------------------

class MemoryQueue:
    """
    Rolling queue of recent embeddings + labels from previous batches.
    Allows contrastive loss to see hundreds of negatives/positives even
    with a small actual batch size — inspired by MoCo.

    Memory cost is tiny: queue_size x embed_dim floats (e.g. 512 x 128 = 256KB).
    """

    def __init__(self, embed_dim, queue_size=512, device="cpu"):
        self.queue_size = queue_size
        self.embed_dim = embed_dim
        self.device = device
        self.embeddings = torch.zeros(queue_size, embed_dim, device=device)
        self.labels = torch.full((queue_size,), -1, dtype=torch.long, device=device)
        self.pair_ids = torch.full((queue_size,), -1, dtype=torch.long, device=device)
        self.ptr = 0
        self.full = False

    @torch.no_grad()
    def enqueue(self, embeddings, labels, pair_ids=None):
        """Add new embeddings to the queue, overwriting oldest entries."""
        batch_size = embeddings.shape[0]
        if pair_ids is None:
            pair_ids = torch.full((batch_size,), -1, dtype=torch.long, device=self.device)

        if batch_size >= self.queue_size:
            self.embeddings = embeddings[-self.queue_size:].detach()
            self.labels = labels[-self.queue_size:].detach()
            self.pair_ids = pair_ids[-self.queue_size:].detach()
            self.ptr = 0
            self.full = True
            return

        end = self.ptr + batch_size
        if end <= self.queue_size:
            self.embeddings[self.ptr:end] = embeddings.detach()
            self.labels[self.ptr:end] = labels.detach()
            self.pair_ids[self.ptr:end] = pair_ids.detach()
        else:
            overflow = end - self.queue_size
            split = batch_size - overflow
            self.embeddings[self.ptr:] = embeddings[:split].detach()
            self.labels[self.ptr:] = labels[:split].detach()
            self.pair_ids[self.ptr:] = pair_ids[:split].detach()
            self.embeddings[:overflow] = embeddings[split:].detach()
            self.labels[:overflow] = labels[split:].detach()
            self.pair_ids[:overflow] = pair_ids[split:].detach()

        self.ptr = end % self.queue_size
        if end >= self.queue_size:
            self.full = True

    def get(self):
        """Return all valid (embeddings, labels, pair_ids) in the queue."""
        if self.full:
            return self.embeddings, self.labels, self.pair_ids
        elif self.ptr > 0:
            return self.embeddings[:self.ptr], self.labels[:self.ptr], self.pair_ids[:self.ptr]
        else:
            return None, None, None

    def size(self):
        return self.queue_size if self.full else self.ptr


class SupConLoss(nn.Module):
    """
    Supervised Contrastive Loss (Khosla et al., 2020) with memory queue support.

    For each anchor, positives are other samples of the same class.
    Negatives are samples of a different class. The memory queue provides
    additional negatives/positives from recent batches, effectively simulating
    a much larger batch size without extra RAM.
    """

    def __init__(self, temperature=0.07):
        super().__init__()
        self.temperature = temperature

    def forward(self, embeddings, labels, pair_ids=None, queue_embeddings=None,
                queue_labels=None, queue_pair_ids=None):
        """
        Args:
            embeddings: (2N, D) — current batch embeddings (both views)
            labels: (2N,) — current batch labels
            pair_ids: (2N,) — instance pair IDs (same billet/position = same ID)
            queue_embeddings: (Q, D) — stored embeddings from previous batches
            queue_labels: (Q,) — stored labels from previous batches
            queue_pair_ids: (Q,) — stored pair IDs from previous batches
        """
        device = embeddings.device
        batch_size = embeddings.shape[0]

        # Combine current batch with queue
        if queue_embeddings is not None and queue_embeddings.shape[0] > 0:
            all_embeddings = torch.cat([embeddings, queue_embeddings.detach()], dim=0)
            all_labels = torch.cat([labels, queue_labels.detach()], dim=0)
            if pair_ids is not None and queue_pair_ids is not None:
                all_pair_ids = torch.cat([pair_ids, queue_pair_ids.detach()], dim=0)
            else:
                all_pair_ids = None
        else:
            all_embeddings = embeddings
            all_labels = labels
            all_pair_ids = pair_ids

        total_size = all_embeddings.shape[0]

        # Similarity matrix: batch anchors vs all
        sim_matrix = torch.matmul(embeddings, all_embeddings.T) / self.temperature

        # Self-similarity mask
        mask_self = torch.zeros(batch_size, total_size, dtype=torch.bool, device=device)
        mask_self[:batch_size, :batch_size] = torch.eye(batch_size, dtype=torch.bool, device=device)
        sim_matrix = sim_matrix.masked_fill(mask_self, -1e9)

        # Positive mask: use pair_ids (instance-level) if available, otherwise class-level
        if all_pair_ids is not None:
            anchor_pids = pair_ids.unsqueeze(1)
            all_pids = all_pair_ids.unsqueeze(0)
            # Instance-level positives: same pair_id = same physical surface location
            mask_instance = (anchor_pids == all_pids) & ~mask_self
            # Class-level positives: same class label
            anchor_labels = labels.unsqueeze(1)
            all_lab = all_labels.unsqueeze(0)
            mask_class = (anchor_labels == all_lab) & ~mask_self
            # Combine: instance pairs are strongest, class-level adds more signal
            # Weight instance matches higher by using them as definite positives
            mask_pos = mask_instance | mask_class
        else:
            anchor_labels = labels.unsqueeze(1)
            all_lab = all_labels.unsqueeze(0)
            mask_pos = (anchor_labels == all_lab) & ~mask_self

        # Numerical stability
        sim_max, _ = sim_matrix.max(dim=1, keepdim=True)
        sim_matrix = sim_matrix - sim_max.detach()

        # Denominator
        exp_sim = torch.exp(sim_matrix) * (~mask_self).float()
        log_sum_exp = torch.log(exp_sim.sum(dim=1, keepdim=True) + 1e-8)

        # Mean of log-prob over positive pairs
        pos_count = mask_pos.float().sum(dim=1, keepdim=True).clamp(min=1.0)
        log_prob_pos = (sim_matrix - log_sum_exp) * mask_pos.float()
        loss = -(log_prob_pos.sum(dim=1) / pos_count.squeeze()).mean()

        return loss


# ---------------------------------------------------------------------------
#  Stage 1: Contrastive Pretraining
# ---------------------------------------------------------------------------

def train_contrastive_epoch(model, loader, criterion, optimizer, device, epoch, total_epochs, queue):
    model.train()
    running_loss = 0.0
    total = 0

    pbar = tqdm(loader, desc=f"  Train {epoch}/{total_epochs}",
                bar_format="{l_bar}{bar:20}{r_bar}", leave=False, file=sys.stdout)

    for batch in pbar:
        view1, view2, labels, pair_ids = batch
        view1, view2 = view1.to(device), view2.to(device)
        labels = labels.to(device)
        pair_ids = pair_ids.to(device)

        _, emb1 = model(view1)
        _, emb2 = model(view2)

        # Concatenate both views
        embeddings = torch.cat([emb1, emb2], dim=0)
        all_labels = torch.cat([labels, labels], dim=0)
        all_pair_ids = torch.cat([pair_ids, pair_ids], dim=0)

        # Get stored embeddings from queue
        q_emb, q_lab, q_pids = queue.get()

        loss = criterion(embeddings, all_labels, pair_ids=all_pair_ids,
                         queue_embeddings=q_emb, queue_labels=q_lab, queue_pair_ids=q_pids)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Update queue
        queue.enqueue(embeddings, all_labels, all_pair_ids)

        running_loss += loss.item() * view1.size(0)
        total += view1.size(0)

        pbar.set_postfix(loss=f"{running_loss / total:.4f}", queue=queue.size())

    pbar.close()
    return running_loss / total


@torch.no_grad()
def eval_contrastive(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    total = 0

    pbar = tqdm(loader, desc="  Val",
                bar_format="{l_bar}{bar:20}{r_bar}", leave=False, file=sys.stdout)

    for batch in pbar:
        view1, view2, labels, pair_ids = batch
        view1, view2 = view1.to(device), view2.to(device)
        labels = labels.to(device)
        pair_ids = pair_ids.to(device)

        _, emb1 = model(view1)
        _, emb2 = model(view2)

        embeddings = torch.cat([emb1, emb2], dim=0)
        all_labels = torch.cat([labels, labels], dim=0)
        all_pair_ids = torch.cat([pair_ids, pair_ids], dim=0)

        loss = criterion(embeddings, all_labels, pair_ids=all_pair_ids)
        running_loss += loss.item() * view1.size(0)
        total += view1.size(0)

    pbar.close()
    return running_loss / total


def run_stage1(args, device, output_dir):
    """Supervised contrastive pretraining with cross-modal pairing."""
    print("\n" + "=" * 70)
    print("  STAGE 1: Cross-Modal Supervised Contrastive Pretraining")
    print("=" * 70)

    # Collect cross-modal pairs
    use_synth = (REPO_ROOT / "synthetic").exists()
    paired, unpaired, stats = collect_cross_modal_pairs(
        use_synthetic=use_synth, no_gelsight=args.no_gelsight)

    print(f"\n  Paired samples (phone+depth):  {stats['paired']}")
    print(f"  Unpaired samples:              {stats['unpaired']}")
    print(f"  Unique positions:              {stats['unique_positions']}")
    print(f"  Total phone images:            {stats['total_phone']}")
    print(f"  Total depth images:            {stats['total_depth']}")
    if use_synth:
        print(f"  (includes synthetic data)")

    if not paired and not unpaired:
        print("ERROR: No training data found.")
        return None

    # Split by pair_id (keeps paired data together, no leakage)
    train_paired, train_unpaired, val_paired, val_unpaired = train_val_split_paired(
        paired, unpaired, val_ratio=args.val_ratio, seed=args.seed)

    train_dataset = CrossModalContrastiveDataset(
        train_paired, train_unpaired, transform=get_contrastive_transform())
    val_dataset = CrossModalContrastiveDataset(
        val_paired, val_unpaired, transform=get_contrastive_transform())

    train_loader = DataLoader(train_dataset, batch_size=args.batch, shuffle=True,
                              num_workers=0, pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch, shuffle=False,
                            num_workers=0, pin_memory=True, drop_last=False)

    model = ContrastiveModel().to(device)
    criterion = SupConLoss(temperature=args.temperature)
    optimizer = optim.AdamW(model.parameters(), lr=args.pretrain_lr, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.pretrain_epochs)

    # Memory queue: stores recent embeddings so the loss sees many more comparisons
    queue = MemoryQueue(embed_dim=EMBED_DIM, queue_size=args.queue_size, device=device)
    effective_batch = args.batch * 2 + args.queue_size  # 2 views + queue

    n_train = len(train_paired) + len(train_unpaired)
    n_val = len(val_paired) + len(val_unpaired)
    n_batches = len(train_loader)
    print(f"\nSamples — train: {n_train}, val: {n_val}")
    print(f"Batches per epoch: {n_batches}")
    print(f"Batch size: {args.batch}, Temperature: {args.temperature}")
    print(f"Memory queue: {args.queue_size} embeddings (effective comparisons: ~{effective_batch})")
    print(f"Learning rate: {args.pretrain_lr}, Epochs: {args.pretrain_epochs}")
    print(f"\n{'Epoch':>5} | {'Train Loss':>10} | {'Val Loss':>10} | {'LR':>10} | {'Time':>8} | {'ETA':>10}")
    print("-" * 70)

    best_val_loss = float("inf")
    history = []
    stage_start = time.time()

    for epoch in range(1, args.pretrain_epochs + 1):
        epoch_start = time.time()
        train_loss = train_contrastive_epoch(model, train_loader, criterion, optimizer, device,
                                             epoch, args.pretrain_epochs, queue)
        val_loss = eval_contrastive(model, val_loader, criterion, device)
        lr = optimizer.param_groups[0]["lr"]
        scheduler.step()

        elapsed = time.time() - epoch_start
        total_elapsed = time.time() - stage_start
        avg_per_epoch = total_elapsed / epoch
        eta = avg_per_epoch * (args.pretrain_epochs - epoch)
        eta_str = f"{int(eta//60)}m{int(eta%60):02d}s" if eta >= 60 else f"{eta:.0f}s"

        print(f"{epoch:5d} | {train_loss:10.4f} | {val_loss:10.4f} | {lr:10.6f} | {elapsed:6.1f}s | {eta_str:>10}")

        history.append({
            "epoch": epoch,
            "train_loss": round(train_loss, 5),
            "val_loss": round(val_loss, 5),
            "lr": round(lr, 8),
        })

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save({
                "epoch": epoch,
                "model_state_dict": model.state_dict(),
                "backbone_state_dict": model.backbone.state_dict(),
                "val_loss": val_loss,
                "feature_dim": model.feature_dim,
                "embed_dim": EMBED_DIM,
                "args": vars(args),
            }, output_dir / "contrastive_best.pt")

    # Always save final
    torch.save({
        "epoch": args.pretrain_epochs,
        "model_state_dict": model.state_dict(),
        "backbone_state_dict": model.backbone.state_dict(),
        "val_loss": val_loss,
        "feature_dim": model.feature_dim,
        "embed_dim": EMBED_DIM,
        "args": vars(args),
    }, output_dir / "contrastive_final.pt")

    with open(output_dir / "contrastive_history.json", "w") as f:
        json.dump(history, f, indent=2)

    print(f"\nStage 1 complete. Best val loss: {best_val_loss:.4f}")
    print(f"Backbone saved to: {output_dir / 'contrastive_best.pt'}")

    return model


# ---------------------------------------------------------------------------
#  Stage 2: Supervised Fine-tuning
# ---------------------------------------------------------------------------

def train_classifier_epoch(backbone, classifier, loader, criterion, optimizer, device,
                           fine_tune_backbone=False, epoch=0, total_epochs=0, phase=""):
    if fine_tune_backbone:
        backbone.train()
    else:
        backbone.eval()
    classifier.train()

    running_loss = 0.0
    correct = 0
    total = 0

    desc = f"  Train {epoch}/{total_epochs} [{phase}]"
    pbar = tqdm(loader, desc=desc,
                bar_format="{l_bar}{bar:20}{r_bar}", leave=False, file=sys.stdout)

    for images, labels in pbar:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()

        with torch.set_grad_enabled(fine_tune_backbone):
            features = backbone(images)
        if isinstance(features, tuple):
            features = features[0]

        outputs = classifier(features.detach() if not fine_tune_backbone else features)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * images.size(0)
        _, predicted = outputs.max(1)
        correct += predicted.eq(labels).sum().item()
        total += labels.size(0)

        pbar.set_postfix(loss=f"{running_loss/total:.4f}", acc=f"{correct/total:.1%}")

    pbar.close()
    return running_loss / total, correct / total


@torch.no_grad()
def eval_classifier(backbone, classifier, loader, criterion, device):
    backbone.eval()
    classifier.eval()

    running_loss = 0.0
    correct = 0
    total = 0

    pbar = tqdm(loader, desc="  Val",
                bar_format="{l_bar}{bar:20}{r_bar}", leave=False, file=sys.stdout)

    for images, labels in pbar:
        images, labels = images.to(device), labels.to(device)
        features = backbone(images)
        if isinstance(features, tuple):
            features = features[0]
        outputs = classifier(features)
        loss = criterion(outputs, labels)

        running_loss += loss.item() * images.size(0)
        _, predicted = outputs.max(1)
        correct += predicted.eq(labels).sum().item()
        total += labels.size(0)

    pbar.close()
    return running_loss / total, correct / total


def run_stage2(args, device, output_dir, contrastive_model=None):
    """Supervised fine-tuning with contrastive-pretrained backbone."""
    print("\n" + "=" * 70)
    print("  STAGE 2: Supervised Fine-tuning")
    print("=" * 70)

    # Load pretrained backbone
    if contrastive_model is not None:
        backbone = contrastive_model.backbone
        feature_dim = contrastive_model.feature_dim
        print("Using backbone from Stage 1 (in-memory)")
    else:
        ckpt_path = output_dir / "contrastive_best.pt"
        if not ckpt_path.exists():
            print(f"ERROR: No pretrained backbone found at {ckpt_path}")
            print("Run Stage 1 first: python train.py --stage 1")
            return
        checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
        feature_dim = checkpoint["feature_dim"]

        backbone = models.efficientnet_b0(weights=None)
        backbone.classifier = nn.Identity()
        backbone.load_state_dict(checkpoint["backbone_state_dict"])
        backbone = backbone.to(device)
        print(f"Loaded pretrained backbone from {ckpt_path}")

    classifier = Classifier(feature_dim, num_classes=2).to(device)

    # Collect all images as flat samples for classification
    use_synth = (REPO_ROOT / "synthetic").exists()
    all_samples = collect_all_flat_samples(use_synthetic=use_synth, no_gelsight=args.no_gelsight)
    print(f"Total images for fine-tuning: {len(all_samples)}")

    train_samples, val_samples = train_val_split(all_samples, val_ratio=args.val_ratio, seed=args.seed)

    train_dataset = BilletSurfaceDataset(train_samples, transform=get_finetune_transform())
    val_dataset = BilletSurfaceDataset(val_samples, transform=get_val_transform())

    sampler = make_weighted_sampler(train_samples)

    train_loader = DataLoader(train_dataset, batch_size=args.batch, sampler=sampler,
                              num_workers=0, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch, shuffle=False,
                            num_workers=0, pin_memory=True)

    # Phase A: frozen backbone, train only classifier head
    # Phase B: unfreeze and fine-tune everything with low LR
    total_epochs = args.finetune_epochs
    freeze_epochs = max(1, total_epochs // 3)
    unfreeze_epochs = total_epochs - freeze_epochs

    print(f"\nSamples — train: {len(train_samples)}, val: {len(val_samples)}")
    print(f"Batches per epoch: {len(train_loader)}")
    print(f"Phase A (frozen backbone): {freeze_epochs} epochs, lr={args.finetune_lr}")
    print(f"Phase B (fine-tune all):   {unfreeze_epochs} epochs, lr={args.finetune_lr / 10}")

    criterion = nn.CrossEntropyLoss()
    best_val_acc = 0.0
    best_epoch = 0
    history = []
    stage_start = time.time()

    # ---- Phase A: Train classifier head only ----
    optimizer = optim.AdamW(classifier.parameters(), lr=args.finetune_lr, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=freeze_epochs)

    print(f"\n{'Epoch':>5} | {'Phase':>8} | {'Train Loss':>10} | {'Train Acc':>9} | {'Val Loss':>10} | {'Val Acc':>9} | {'Time':>8} | {'ETA':>10}")
    print("-" * 90)

    for epoch in range(1, freeze_epochs + 1):
        epoch_start = time.time()
        train_loss, train_acc = train_classifier_epoch(
            backbone, classifier, train_loader, criterion, optimizer, device,
            fine_tune_backbone=False, epoch=epoch, total_epochs=total_epochs, phase="frozen")
        val_loss, val_acc = eval_classifier(backbone, classifier, val_loader, criterion, device)
        scheduler.step()

        elapsed = time.time() - epoch_start
        total_elapsed = time.time() - stage_start
        avg_per_epoch = total_elapsed / epoch
        eta = avg_per_epoch * (total_epochs - epoch)
        eta_str = f"{int(eta//60)}m{int(eta%60):02d}s" if eta >= 60 else f"{eta:.0f}s"

        phase = "frozen"
        print(f"{epoch:5d} | {phase:>8} | {train_loss:10.4f} | {train_acc:8.1%} | {val_loss:10.4f} | {val_acc:8.1%} | {elapsed:6.1f}s | {eta_str:>10}")

        history.append({
            "epoch": epoch, "phase": phase,
            "train_loss": round(train_loss, 5), "train_acc": round(train_acc, 4),
            "val_loss": round(val_loss, 5), "val_acc": round(val_acc, 4),
        })

        if val_acc >= best_val_acc:
            best_val_acc = val_acc
            best_epoch = epoch
            _save_classifier(backbone, classifier, epoch, val_acc, val_loss, args, output_dir)

    # ---- Phase B: Fine-tune backbone + classifier ----
    all_params = list(backbone.parameters()) + list(classifier.parameters())
    optimizer = optim.AdamW(all_params, lr=args.finetune_lr / 10, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=unfreeze_epochs)

    for epoch in range(freeze_epochs + 1, total_epochs + 1):
        epoch_start = time.time()
        train_loss, train_acc = train_classifier_epoch(
            backbone, classifier, train_loader, criterion, optimizer, device,
            fine_tune_backbone=True, epoch=epoch, total_epochs=total_epochs, phase="finetune")
        val_loss, val_acc = eval_classifier(backbone, classifier, val_loader, criterion, device)
        scheduler.step()

        elapsed = time.time() - epoch_start
        total_elapsed = time.time() - stage_start
        avg_per_epoch = total_elapsed / epoch
        eta = avg_per_epoch * (total_epochs - epoch)
        eta_str = f"{int(eta//60)}m{int(eta%60):02d}s" if eta >= 60 else f"{eta:.0f}s"

        phase = "finetune"
        print(f"{epoch:5d} | {phase:>8} | {train_loss:10.4f} | {train_acc:8.1%} | {val_loss:10.4f} | {val_acc:8.1%} | {elapsed:6.1f}s | {eta_str:>10}")

        history.append({
            "epoch": epoch, "phase": phase,
            "train_loss": round(train_loss, 5), "train_acc": round(train_acc, 4),
            "val_loss": round(val_loss, 5), "val_acc": round(val_acc, 4),
        })

        if val_acc >= best_val_acc:
            best_val_acc = val_acc
            best_epoch = epoch
            _save_classifier(backbone, classifier, epoch, val_acc, val_loss, args, output_dir)

    with open(output_dir / "finetune_history.json", "w") as f:
        json.dump(history, f, indent=2)

    print(f"\nStage 2 complete.")
    print(f"Best val accuracy: {best_val_acc:.1%} (epoch {best_epoch})")
    print(f"Model saved to: {output_dir / 'best_model.pt'}")


def _save_classifier(backbone, classifier, epoch, val_acc, val_loss, args, output_dir):
    """Save the full model (backbone + classifier) for inference."""
    torch.save({
        "epoch": epoch,
        "backbone_state_dict": backbone.state_dict(),
        "classifier_state_dict": classifier.state_dict(),
        "val_acc": val_acc,
        "val_loss": val_loss,
        "class_names": CLASS_NAMES,
        "label_map": LABEL_MAP,
        "img_size": IMG_SIZE,
        "args": vars(args),
    }, output_dir / "best_model.pt")


# ---------------------------------------------------------------------------
#  Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(description="Train billet surface classifier (contrastive + fine-tune)")

    # Stage selection
    parser.add_argument("--stage", type=int, default=0, choices=[0, 1, 2],
                        help="0=both stages (default), 1=contrastive only, 2=fine-tune only")

    # Data
    parser.add_argument("--no-gelsight", action="store_true", help="Skip GelSight data, phone only")
    parser.add_argument("--val-ratio", type=float, default=0.2, help="Validation split ratio")

    # Stage 1: Contrastive pretraining
    parser.add_argument("--pretrain-epochs", type=int, default=60,
                        help="Number of contrastive pretraining epochs")
    parser.add_argument("--pretrain-lr", type=float, default=3e-4,
                        help="Learning rate for contrastive pretraining")
    parser.add_argument("--temperature", type=float, default=0.07,
                        help="Temperature for SupCon loss")
    parser.add_argument("--queue-size", type=int, default=512,
                        help="Memory queue size for contrastive learning (stores recent embeddings)")

    # Stage 2: Fine-tuning
    parser.add_argument("--finetune-epochs", type=int, default=30,
                        help="Number of fine-tuning epochs (split into frozen + unfreeze phases)")
    parser.add_argument("--finetune-lr", type=float, default=1e-3,
                        help="Learning rate for classifier fine-tuning")

    # General
    parser.add_argument("--batch", type=int, default=16, help="Batch size")
    parser.add_argument("--output-dir", type=str, default="trained_models",
                        help="Directory to save trained models")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")

    args = parser.parse_args()

    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\n{'='*70}")
    print(f"  Billet Surface Classifier — Contrastive Learning Pipeline")
    print(f"{'='*70}")
    print(f"  Device:  {device}")
    print(f"  Stage:   {'both' if args.stage == 0 else args.stage}")
    print(f"  Seed:    {args.seed}")
    print(f"{'='*70}")

    # Check for synthetic data
    use_synth = (REPO_ROOT / "synthetic").exists()
    if use_synth:
        print(f"  Synthetic data: found")
    else:
        print(f"  Synthetic data: not found (run generate_synthetic.py first for best results)")

    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)

    # ---- Run stages ----
    overall_start = time.time()
    contrastive_model = None

    if args.stage in (0, 1):
        contrastive_model = run_stage1(args, device, output_dir)

    if args.stage in (0, 2):
        run_stage2(args, device, output_dir, contrastive_model=contrastive_model)

    total_time = time.time() - overall_start
    mins, secs = divmod(int(total_time), 60)
    print(f"\nAll done! Total time: {mins}m {secs:02d}s")


if __name__ == "__main__":
    main()
